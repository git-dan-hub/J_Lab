{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create custom deployments with `ibm-watson-machine-learning`\n",
    "\n",
    "This notebook facilitates Pytorch ML library in Watson Machine Learning service. It contains steps and code to work with [ibm-watson-machine-learning](https://pypi.python.org/pypi/ibm-watson-machine-learning) library available in PyPI repository. It also introduces commands for creating and deploying a custom fucntion.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "-  Create a custom python function \n",
    "-  Persist the function model in Watson Machine Learning repository.\n",
    "-  Deploy function for online scoring using client library.\n",
    "-  Score sample records using client library.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Setup](#setup)\n",
    "2.\t[Create a custom python function](#download)\n",
    "3.\t[Persist the function model](#persistence)\n",
    "4.\t[Deploy and score in a Cloud](#scoring)\n",
    "5.  [Clean up](#cleanup)\n",
    "6.\t[Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a <a href=\"https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to WML\n",
    "\n",
    "Authenticate the Watson Machine Learning service on IBM Cloud. You need to provide platform `api_key` and instance `location`.\n",
    "\n",
    "You can use [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) to retrieve platform API Key and instance location.\n",
    "\n",
    "API Key can be generated in the following way:\n",
    "```\n",
    "ibmcloud login\n",
    "ibmcloud iam api-key-create API_KEY_NAME\n",
    "```\n",
    "\n",
    "In result, get the value of `api_key` from the output.\n",
    "\n",
    "\n",
    "Location of your WML instance can be retrieved in the following way:\n",
    "```\n",
    "ibmcloud login --apikey API_KEY -a https://cloud.ibm.com\n",
    "ibmcloud resource service-instance WML_INSTANCE_NAME\n",
    "```\n",
    "\n",
    "In result, get the value of `location` from the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Your `Cloud API key` can be generated by going to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users). From that page, click your name, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**. Give your key a name and click **Create**, then copy the created key and paste it below.\n",
    "\n",
    "You can also get service specific apikey by going to the [**Service IDs** section of the Cloud Console](https://cloud.ibm.com/iam/serviceids).  From that page, click **Create**, then copy the created key and paste it below.\n",
    "\n",
    "**Action**: Enter your `api_key` and `location` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''\n",
    "location = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_credentials = {\n",
    "    \"apikey\": api_key,\n",
    "    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import the `ibm-watson-machine-learning` package\n",
    "**Note:** `ibm-watson-machine-learning` documentation can be found <a href=\"http://ibm-wml-api-pyclient.mybluemix.net/\" target=\"_blank\" rel=\"noopener no referrer\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: ibm-watson-machine-learning in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.0.22)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-watson-machine-learning) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-watson-machine-learning) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: lomond in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-watson-machine-learning) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: pandas<=1.0.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-watson-machine-learning) (0.24.1)\n",
      "Requirement already satisfied, skipping upgrade: ibm-cos-sdk==2.7.* in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-watson-machine-learning) (2.7.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-watson-machine-learning) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-watson-machine-learning) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from lomond->ibm-watson-machine-learning) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas<=1.0.5->ibm-watson-machine-learning) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas<=1.0.5->ibm-watson-machine-learning) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas<=1.0.5->ibm-watson-machine-learning) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: ibm-cos-sdk-core==2.7.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk==2.7.*->ibm-watson-machine-learning) (2.7.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk==2.7.*->ibm-watson-machine-learning) (0.9.3)\n",
      "Requirement already satisfied, skipping upgrade: ibm-cos-sdk-s3transfer==2.7.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk==2.7.*->ibm-watson-machine-learning) (2.7.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->ibm-watson-machine-learning) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->ibm-watson-machine-learning) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk-core==2.7.0->ibm-cos-sdk==2.7.*->ibm-watson-machine-learning) (0.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ibm-watson-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-09 13:12:11,714 - ibm_watson_machine_learning.wml_client_error - WARNING - deployment deletion failed. Reason: {\"trace\":\"9326f9277c0d3d69ee40430a99d350f2\",\"errors\":[{\"code\":\"deployment_does_not_exist\",\"message\":\"Deployment of name '89098410-5ed9-4d55-9f45-8b81fa829e0f' doesn't exist. Cannot retrieve non existing deployment\"}]} \n",
      "2020-10-09 13:27:34,041 - ibm_watson_machine_learning.wml_client_error - WARNING - Artifact with artifact_uid: '7aab0814-148c-491e-b6dd-dc8b61173eb2' does not exist.\n",
      "2020-10-09 13:29:03,772 - ibm_watson_machine_learning.wml_client_error - WARNING - deployment deletion failed. Reason: {\"trace\":\"2d98dc9ecd06902110ad2bf1a43b7d01\",\"errors\":[{\"code\":\"deployment_does_not_exist\",\"message\":\"Deployment of name '89098410-5ed9-4d55-9f45-8b81fa829e0f' doesn't exist. Cannot retrieve non existing deployment\"}]} \n",
      "2020-10-09 13:41:51,691 - ibm_watson_machine_learning.wml_client_error - WARNING - Failure during scoring. (POST https://us-south.ml.cloud.ibm.com/ml/v4/deployments/4dac3e01-26c7-4e06-bcce-3ef01e97c260/predictions?version=2020-08-01)\n",
      "Status code: 400, body: {\"trace\": \"5ae4803a1a258c5950feee247a2421ab\", \"errors\": [{\"code\": \"score_processing_failure\", \"message\": \"too many dimensions 'str'\"}], \"status_code\": 400}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with spaces\n",
    "\n",
    "First of all, you need to create a space that will be used for your work. If you do not have space already created, you can use [Deployment Spaces Dashboard](https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas) to create one.\n",
    "\n",
    "- Click New Deployment Space\n",
    "- Create an empty space\n",
    "- Select Cloud Object Storage\n",
    "- Select Watson Machine Learning instance and press Create\n",
    "- Copy `space_id` and paste it below\n",
    "\n",
    "**Tip**: You can also use SDK to prepare the space for your work. More information can be found [here](https://github.com/IBM/watson-machine-learning-samples/blob/master/notebooks/python_sdk/instance-management/Space%20management.ipynb).\n",
    "\n",
    "**Action**: Assign space ID below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_id = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.spaces.get_details(space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `list` method to print all existing spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.spaces.list(limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to interact with all resources available in Watson Machine Learning, you need to set **space** which you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"download\"></a>\n",
    "## 2. Create a custom python function\n",
    "In this section, you will create a custom python function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python fucnctions for deployment needs to have this specific format for payload_input. It is a dictionary with the key \"input_data\" and the key \"values\" that contains the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records= ['Record1' , 'Record2' , 'Record3']\n",
    "payload_input = {\"input_data\": [{\"values\": records}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payload_output for deployment needs to be a dictionary with the key \"predictions\" and the key \"values\" which would contain the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['Record1-changed' , 'Record2-changed' , 'Record3-changed']\n",
    "payload_output = {\"predictions\": [{\"values\": outputs}]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Simple python function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will see how to create a simple python function for deployment within WML.\n",
    "This function takes the strings in the record and adds an exclaimation to the end of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addexclaimation():\n",
    "    \n",
    "    def exclaimation(payload_input):\n",
    "        outputs =[]\n",
    "        \n",
    "        records = payload_input.get(\"input_data\")[0].get(\"values\")\n",
    "        for record in records :\n",
    "            #This is where we can preprocess each record\n",
    "            outputs.append(record+\"!\")\n",
    "               \n",
    "        payload_output= {\"predictions\": [{\"values\": outputs}]}    \n",
    "        return payload_output\n",
    "    \n",
    "    return exclaimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the functions using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\"Hello\" ,\"How are you doing\"]\n",
    "payload_input = {\"input_data\": [{\"values\": records}]}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_result = addexclaimation()(payload_input)\n",
    "print(function_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Python function with external libraries for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will see how to preprocess records with PyTorch torchtext using python functions using WML.\n",
    "This function takes the strings in the record and converts them into a list of letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using external libraries we need to install and import them within the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPreprocessing() :\n",
    "    \"\"\"\n",
    "    Install the external libraries\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    def install(name):\n",
    "        subprocess.call(['pip', 'install', name])\n",
    "\n",
    "    install(\"torch=1.6\")    \n",
    "    install(\"torchtext\")\n",
    "\n",
    "    \"\"\"\n",
    "    Import the external libraries\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torchtext\n",
    "    \n",
    "    def preprocessing(payload_input) :\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "    \n",
    "        records = payload_input.get(\"input_data\")[0].get(\"values\")\n",
    "        outputs =[]\n",
    "        \n",
    "        TEXT = torchtext.data.Field(lower = True, batch_first = True, init_token  = '<bos>', eos_token   = '<eos>',)\n",
    "\n",
    "        for record in records:\n",
    "            record=  list(map(TEXT.preprocess, record))\n",
    "            outputs.append(record)\n",
    "            \n",
    "        payload_output = {\"predictions\": [{\"values\": outputs}]}    \n",
    "       \n",
    "        return payload_output\n",
    "    \n",
    "    return preprocessing    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the functions using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\"Bat\" ,\"Cat\"]\n",
    "payload_input = {\"input_data\": [{\"values\": records}]}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'values': [[['b'], ['a'], ['t']], [['c'], ['a'], ['t']]]}]}\n"
     ]
    }
   ],
   "source": [
    "function_result = addPreprocessing()(payload_input)\n",
    "print(function_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Python function with external libraries for preprocessing and deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPreprocessingandScoring() :\n",
    "    \"\"\"\n",
    "    Install the external libraries\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    def install(name):\n",
    "        subprocess.call(['pip', 'install', name])\n",
    "\n",
    "    install(\"torch=1.6\")    \n",
    "    install(\"torchtext\")\n",
    "    install(\"torchvision\")\n",
    "    install(\"wget\")\n",
    "    \n",
    "    \"\"\"\n",
    "    Import the external libraries\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torchtext\n",
    "    import os\n",
    "    import wget\n",
    "    import pickle\n",
    "    import types\n",
    "    import pandas as pd\n",
    "    from botocore.client import Config\n",
    "    import ibm_boto3\n",
    "    import io\n",
    "    from torchvision import datasets, transforms\n",
    "    import torch.nn.functional as F\n",
    "        \n",
    "    \n",
    "    def preprocessingandscoring(payload_input) :\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        Read in saved .pkl for data assests\n",
    "        \"\"\"\n",
    "        clients3 = ibm_boto3.client(service_name='s3',\n",
    "            ibm_api_key_id='dKHIg3RBJ-Wais5slCJ69xifQZ_Vt1XhTxtGgr3qpH2I',\n",
    "            ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "            config=Config(signature_version='oauth'),\n",
    "            endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "        streamingbody = clients3.get_object(Bucket='vmwareenablement-donotdelete-pr-dfpqt0z6tem3ox', Key='model.pkl')['Body']\n",
    "        model_path = io.BytesIO(streamingbody.read())\n",
    "     \n",
    "        \n",
    "        \"\"\"\n",
    "        Define model corresponding to the saved weights\n",
    "        \"\"\"\n",
    "        \n",
    "        class Net(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Net, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "                self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "                self.conv2_drop = nn.Dropout2d()\n",
    "                self.fc1 = nn.Linear(320, 50)\n",
    "                self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "                x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "                x = x.view(-1, 320)\n",
    "                x = F.relu(self.fc1(x))\n",
    "                x = F.dropout(x, training=self.training)\n",
    "                x = self.fc2(x)\n",
    "                return F.log_softmax(x)\n",
    "            \n",
    "        \"\"\"\n",
    "        Load model corresponding to the saved weights\n",
    "        \"\"\"    \n",
    "        \n",
    "        device = torch.device(\"cpu\")\n",
    "        model = Net()\n",
    "        model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "        model.eval()\n",
    "        \n",
    "        \"\"\"\n",
    "        Preprocess records and get predictions\n",
    "        \"\"\" \n",
    "    \n",
    "        records = payload_input.get(\"input_data\")[0].get(\"values\")\n",
    "        records = torch.tensor(records) \n",
    "        \n",
    "        outputs= model(records.float())    \n",
    "        predictions = [a.index(max(a)) for a in outputs.tolist()]\n",
    "            \n",
    "        payload_output = {\"predictions\": [{\"values\": predictions}]}    \n",
    "       \n",
    "        return payload_output\n",
    "    \n",
    "    return preprocessingandscoring    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this function lets download this MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /opt/conda/envs/Python36/lib/python3.6/site-packages (3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "import os\n",
    "import wget\n",
    "import numpy as np\n",
    "\n",
    "data_dir = 'MNIST_DATA'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "filename = os.path.join(data_dir, 'mnist.npz')\n",
    "if not os.path.isfile(filename):\n",
    "    wget.download('https://s3.amazonaws.com/img-datasets/mnist.npz', out=data_dir) \n",
    "    \n",
    "dataset =  np.load(filename)\n",
    "x_test = dataset['x_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, image in enumerate([x_test[0], x_test[1]]):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [[x_test[0].tolist()] ,[x_test[1].tolist()]]\n",
    "payload_input = {\"input_data\": [{\"values\": records}]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'values': [7, 2]}]}\n"
     ]
    }
   ],
   "source": [
    "function_result = addPreprocessingandScoring()(payload_input)\n",
    "print(function_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"persistence\"></a>\n",
    "## 3. Persist custom python function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will learn how to store your function in Watson Machine Learning repository by using the IBM Watson Machine Learning SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Publish functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publish function in Watson Machine Learning repository on Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model name,and software_specifications for the python function would be 'ai-function_0.1-py3.6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sofware_spec_uid_function = client.software_specifications.get_id_by_name(\"ai-function_0.1-py3.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "function_meta_props = {\n",
    "         client.repository.FunctionMetaNames.NAME: 'Sample Python Function Deployment',\n",
    "         client.repository.FunctionMetaNames.SOFTWARE_SPEC_ID: sofware_spec_uid_function\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_artifact = client.repository.store_function(meta_props=function_meta_props, function=addPreprocessingandScoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Get function details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_uid = client.repository.get_function_id(function_artifact)\n",
    "print(\"Function UID = \" + function_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Get all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  ----  -------  ----\n",
      "GUID  NAME  CREATED  TYPE\n",
      "----  ----  -------  ----\n"
     ]
    }
   ],
   "source": [
    " client.repository.list_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #client.repository.delete('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scoring\"></a>\n",
    "## 4. Deploy and score in a Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will learn how to create online scoring and to score a new data record by using the IBM Watson Machine Learning SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Create function deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create online deployment for published function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the hardware specifications available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------  ------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "NAME           ID                                    DESCRIPTION\n",
      "ML             7a46ecc7-0ea9-4c94-a444-fbda4d6adc4a  A hardware specification providing 4 CPU cores and 32 GiB of memory.\n",
      "V100x2         a02f3ab5-6964-4f06-a870-c7cc69187895  A hardware specification providing 52 CPU cores and 96 GiB of memory with 2 Nvidia v100 GPUs.\n",
      "L              a6c4923b-b8e4-444c-9f43-8a7ec3020110  A hardware specification providing 8 CPU cores and 32 GiB of memory.\n",
      "Default Spark  ac59d20a-9c7c-4504-a853-788ef35969da  A hardware specification for Spark with 1 CPU and 4 GiB of memory for master and worker nodes, with 2 workers.\n",
      "XXS            b128f957-581d-46d0-95b6-8af5cd5be580  A hardware specification providing one CPU core and 2 GiB of memory.\n",
      "M-Spark        b2232f7a-bfad-4822-9bce-6ba1af49217a  A hardware specification for Spark service with 2 CPU and 8 GiB of memory for master and 2 CPU and 8 GiB of memory for worker nodes.\n",
      "V100x4         b305a34b-acb5-4850-a44a-f1f15e304a20  A hardware specification providing 104 CPU cores and 192 GiB of memory with 4 Nvidia v100 GPUs.\n",
      "M              c076e82c-b2a7-4d20-9c0f-1f0c2fdf5a24  A hardware specification providing 4 CPU cores and 16 GiB of memory.\n",
      "XL-Spark       c1791762-1333-4dd3-b7bb-228ae287da31  A hardware specification for Spark service with 3 CPU and 12 GiB of memory for master and 4 CPU and 12 GiB of memory for worker nodes.\n",
      "K80            cf70f086-916d-4684-91a7-264c49c6d425  A hardware specification providing 4 CPU cores and 48 GiB of memory with 1 Nvidia K80 GPU.\n",
      "XL             d0aa1ae8-a889-42e2-a099-041b604b9289  A hardware specification providing 16 CPU cores and 64 GiB of memory.\n",
      "K80x2          d0f52aa1-4312-40f6-ad84-f16cf5c6da9e  A hardware specification providing 8 CPU cores and 96 GiB of memory with 2 Nvidia K80 GPUs.\n",
      "S-Spark        d92943ba-9f47-407d-9280-c85281687a1e  A hardware specification for Spark service with 1 CPU and 4 GiB of memory for master and 2 CPU and 4 GiB of memory for worker nodes.\n",
      "XS-Spark       e18b1866-e8fa-49c8-9aa5-dfaaed6ffa43  A hardware specification for Spark with 1 CPU and 4 GiB of memory for master and worker nodes.\n",
      "S              e7ed1d6c-2e89-42d7-aed5-863b972c1d2b  A hardware specification providing 2 CPU cores and 8 GiB of memory.\n",
      "K80x4          ec104857-0389-4649-af8e-971fc11982d0  A hardware specification providing 16 CPU cores and 192 GiB of memory with 4 Nvidia K80 GPUs.\n",
      "L-Spark        f132f14a-6c0f-4570-b87c-98ad1e297953  A hardware specification for Spark service with 2 CPU and 8 GiB of memory for master and 4 CPU and 9 GiB of memory for worker nodes.\n",
      "V100           f327bdf7-5634-43d8-b1e3-445afeaf18b9  A hardware specification providing 26 CPU cores and 48 GiB of memory with 1 Nvidia v100 GPU.\n",
      "XS             f3ebac7d-0a75-410c-8b48-a931428cc4c5  A hardware specification providing one CPU core and 4 GiB of memory.\n",
      "-------------  ------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "client.hardware_specifications.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardware_spec_id = client.hardware_specifications.get_id_by_name('K80x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "function_deploy_meta = {\n",
    "     client.deployments.ConfigurationMetaNames.NAME: \"Test Preprocess Deployment\",\n",
    "     client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
    "     client.deployments.ConfigurationMetaNames.HARDWARE_SPEC: { \"id\": hardware_spec_id}\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "Synchronous deployment creation for uid: '7903e9e5-ce59-4948-9b88-a69d0cad0f85' started\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "\n",
      "initializing............\n",
      "ready\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Successfully finished deployment creation, deployment_uid='4dac3e01-26c7-4e06-bcce-3ef01e97c260'\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function_deployment_details = client.deployments.create(function_uid, meta_props=function_deploy_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can print an online scoring endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_endpoint = client.deployments.get_scoring_href(function_deployment_details)\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also list existing deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  ----  -----  -------\n",
      "GUID  NAME  STATE  CREATED\n",
      "----  ----  -----  -------\n"
     ]
    }
   ],
   "source": [
    "client.deployments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#client.deployments.delete('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Get deployment details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_deployment_uid = function_deployment_details['metadata']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.deployments.get_details(function_deployment_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use below method to do test scoring request against deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = {\"input_data\": [{\"values\": [\"Lets test it\" , \"It works\"]}]}\n",
    "input = {\"input_data\": [{\"values\": [[x_test[0].tolist()] ,[x_test[1].tolist()]]}]}\n",
    "#input = {\"input_data\": [{\"values\": [[x_test[2].tolist()] ,[x_test[3].tolist()],[x_test[4].tolist()],[x_test[5].tolist()]]}]}\n",
    "#input = {\"input_data\": [{\"values\": [[x_test[6].tolist()] ,[x_test[7].tolist()],[x_test[8].tolist()],[x_test[9].tolist()]]}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'values': [7, 2]}]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.deployments.score(function_deployment_uid, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cleanup\"></a>\n",
    "## 5. Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to clean up all created assets:\n",
    "- experiments\n",
    "- trainings\n",
    "- pipelines\n",
    "- model definitions\n",
    "- models\n",
    "- functions\n",
    "- deployments\n",
    "\n",
    "please follow up this sample [notebook](https://github.com/IBM/watson-machine-learning-samples/blob/master/notebooks/python_sdk/instance-management/Machine%20Learning%20artifacts%20management.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 6. Summary and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You successfully completed this notebook! You learned how to use Pytorch machine learning library as well as Watson Machine Learning for model creation and deployment. Check out our _[Online Documentation](https://console.ng.bluemix.net/docs/services/PredictiveModeling/index.html?pos=2)_ for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "**Sheetal Reddy**, Data Scientist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© 2020 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
